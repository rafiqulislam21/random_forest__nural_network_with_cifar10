# -*- coding: utf-8 -*-
"""islam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h44SmyZXBjBK6TjWIOk1Q8WLlhMiVgPu

# Lab Assignment
Name: **Md Rafiqul Islam** ; 
student id : **12123971**
"""

import ssl
ssl._create_default_https_context = ssl._create_unverified_context #to avoid certificate verify failed error
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

"""# 1 Dataset
(a)
"""

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

x_train_shape = x_train.shape

#reshape y_train and y_test to 1d array
y_train = y_train.reshape(-1,)
y_test = y_test.reshape(-1,)

#Number of training and test samples
no_of_training = len(x_train)
no_of_test_samples = len(x_test)
print("Number of training sample: {}".format(no_of_training))
print("Number of test sample: {}".format(no_of_test_samples))

#Size of the images
print("\nSize of the images: {}X{}".format(x_train_shape[1],x_train_shape[2]))

#Number of color channels
print("\nNumber of color channels: {}".format(x_train_shape[3]))

#Number of classes
unique_classes = np.unique(y_train)
print("\nNumber of classes: {}\n ".format(len(unique_classes)))

#Number of samples for each class
for clss in unique_classes:
  count = (y_train == clss).sum()
  print("Samples in {} class = {} ".format(clss, count))
print('')

#Class label corresponding to each class
class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
for clss in unique_classes:
  print("Class {} is {}".format(clss, class_labels[clss]))

"""(b)"""

x_train_norm = x_train/255
x_test_norm = x_test/255

"""(c)"""

# For each class, showing 5 images.

for class_index in unique_classes:
  fig, ax = plt.subplots(nrows=1,ncols=5)
  plot_counter = 0
  photo_index = 0
  print("{} : {}".format(class_index, class_labels[class_index])) 
  while(plot_counter < 5):
    if(y_train[photo_index] == class_index):
      plt.sca(ax[plot_counter]); 
      plt.imshow((x_train[photo_index])) 
      plt.axis('off')
      plot_counter += 1
    photo_index +=1
  plt.show()

# For each class, showing average images.

fig2 = plt.figure(figsize = (14,14))
for class_index in unique_classes:
    fig2.add_subplot(1,10,class_index+1)
    index_y_current_class = np.where(y_train == class_index)[0]
    plt.xlabel(class_labels[class_index])
    plt.imshow(np.average(x_train_norm[index_y_current_class], axis=0))
    # plt.axis('off')
    plt.tick_params(left=False, top=False, right=False, bottom=False, labelleft=False, labeltop=False, labelright=False, labelbottom=False)
plt.show()

"""(d)"""

#chance level of this dataset
print("Chance level of this dataset is : 1/{} = {}".format(len(unique_classes), 1/len(unique_classes)))
#Here each class have same probability and we have total 10 classes that's why chance level is 1/10.

"""# 2 Random forest
(a)
"""

# Train a random forest with 1000 trees on the training dataset
import numpy as np
import pandas as pd
import seaborn as sb
import sklearn
from sklearn.ensemble import RandomForestClassifier

x_train_norm_reshape = x_train_norm.reshape(50000, 32 * 32 * 3)
y_train_norm_reshape  = y_train.flatten()

random_seed = np.random.seed(1111)
rand_forest_clss = RandomForestClassifier(n_estimators=1000, verbose=2, n_jobs=-1)
rand_forest_model = rand_forest_clss.fit(x_train_norm_reshape, y_train_norm_reshape)

#ref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

"""(b)"""

# predict the labels of the test set
x_test_norm_reshape  = x_test_norm.reshape(10000, 32 * 32 * 3)
y_predicted = rand_forest_clss.predict(x_test_norm_reshape)
y_predicted

"""(c)"""

rand_forest_accuracy = sklearn.metrics.accuracy_score(y_test, y_predicted)
print("Accuracy of the classifier: ", rand_forest_accuracy)

"""(d)"""

c_matrix = sklearn.metrics.confusion_matrix(y_test, y_predicted)
df_c_matrix = pd.DataFrame(c_matrix, index=[clss for clss in class_labels], columns=[clss for clss in class_labels])
print(df_c_matrix)
plt.figure(figsize=(10, 8))
sb.heatmap(df_c_matrix, annot=True)
# plt.xlabel('Predicted')
# plt.ylabel('Real')
plt.show()

"""(e)"""

print("From confusion matrix we can see that there are some confusions. \n"
"Some good examples of confusion is confusion between: \n 1)trucks and automobiles,\n 2)cats and dogs,\n 3)planes and ships etc."
"\nHere those classes which have similar color patterns and shapes are most often confused with one to another")

"""# 3 Neural networks
(a)
"""

import numpy as np
import tensorflow as tf
from keras import models, layers
from tensorflow.keras.utils import to_categorical

y_train_cat = to_categorical(y_train.flatten(), num_classes=10)
y_test_cat = to_categorical(y_test.flatten(), num_classes=10)

np.random.seed(2222)
tf.random.set_seed(2222)

model = models.Sequential([
    layers.Flatten(input_shape=(32, 32, 3)),
    layers.Dense(20, activation='relu'),
    layers.Dense(10)
])

"""(b)"""

opt = tf.keras.optimizers.SGD(learning_rate=0.01)
lss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

model.compile(optimizer=opt, loss=lss, metrics=["accuracy"])

"""(c)"""

model.summary()
total_parameters = model.count_params()
print("\nTotal parameters: {}".format(total_parameters))

"""(d)"""

mdl_fit = model.fit(x_train_norm, y_train_cat, epochs=30, validation_split=0.2)

plt.plot(mdl_fit.history["accuracy"], label="training")
plt.plot(mdl_fit.history["val_accuracy"], label="validation")
plt.title("Accuracy of a fully coupled model with 20 neurons in the hidden layer")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

print("No, the model doesn't overfit. Here we can see the validation accuracy is increasing." 
      "\nSince there is only around 0.4522 val_accuracy, Yes, it makes sense to train the model for more epochs."
      "\nHowever, the increase in accuracy is  slowing down after 15, which means that accuracy is not increasing significantly.")

"""(e)"""

# (e).a

np.random.seed(3333)
tf.random.set_seed(3333)

model = models.Sequential([
    layers.Flatten(input_shape=(32, 32, 3)),
    layers.Dense(100, activation='relu'),
    layers.Dense(10)
])

#===============================================================================

# (e).b

opt = tf.keras.optimizers.SGD(learning_rate=0.01)
lss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

model.compile(optimizer=opt, loss=lss, metrics=["accuracy"])

#===============================================================================

# (e).c

model.summary()
print("\nTotal parameter: {}".format(model.count_params()))


#===============================================================================

# (e).d

mdl_fit = model.fit(x_train_norm, y_train_cat, epochs=30, validation_split=0.2)

plt.plot(mdl_fit.history["accuracy"], label="training")
plt.plot(mdl_fit.history["val_accuracy"], label="validation")
plt.title("Accuracy of a fully coupled model with 100 neurons in the hidden layer")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""(f)"""

# (f).a

tf.random.set_seed(4444)
np.random.seed(4444)

model = models.Sequential([
    #CNN
    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    #pooling
    layers.MaxPooling2D((3, 3)),
    #dense
    layers.Flatten(),
    layers.Dense(10)
])

#===============================================================================

# (f).b

opt = tf.keras.optimizers.SGD(learning_rate=0.01)
lss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

model.compile(optimizer=opt, loss=lss, metrics=['accuracy'])

#===============================================================================

# (f).c

model.summary()
print("Total parameter: {}".format(model.count_params()))

#===============================================================================

# (f).d

mdl_fit = model.fit(x_train_norm, y_train_cat, epochs=30, validation_split=0.2)
plt.plot(mdl_fit.history["accuracy"], label="training")
plt.plot(mdl_fit.history["val_accuracy"], label="validation")
plt.title("Accuracy for convolutional model")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

print("In this case accuracy is better than previous. The two layers are described below.")

print("Convolutional layer: \n"
"Here, the first layer is called the convolutional layer. Essentially, a convolutional layer is a layer with filters "
"\napplied to the original image or  other feature maps of the Deep CNN. This is where most of the custom parameters are found on the network. "
"\nThe most important parameters are the number of kernels and the size of the kernels. This layer takes the previous set of feature maps "
"\nas input and outputs one 2D map for each filter.")

print("Pooling layer: \n"
"The second layer is the pooling layer. A pooling layer is similar to a Convolutional layer but performs certain functions, such as max-pooling, "
"\nwhich takes the maximum value in a certain filter region, or average pooling, which takes the average value in a filter region. Typically used "
"\nto reduce the dimensionality of a network. This layer takes the previous set of feature maps as input, outputs one 2D map for each filter, and "
"\nalso reduces the spatial dimensions.")

"""(g)"""

# (g).a

np.random.seed(5555)
tf.random.set_seed(5555)

model = tf.keras.models.Sequential([
    #Convolutional layer
    tf.keras.layers.Conv2D(32, 3, padding='same', input_shape=(32, 32, 3), activation='relu'),
    #relu
    tf.keras.layers.Conv2D(32, 3, activation='relu'),
    #pooling
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Dropout(0.25),

    #Convolutional layer
    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
    #relu
    tf.keras.layers.Conv2D(64, 3, activation='relu'),
    #pooling
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Dropout(0.25),

    #dense
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='softmax'),
])

#===============================================================================

# (g).b

opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-06)
lss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

model.compile(optimizer=opt, loss=lss, metrics=['accuracy'])

#===============================================================================

# (g).c

model.summary()
print("\nTotal parameter: {}".format(model.count_params()))

#===============================================================================

# (g).d

mdl_fit = model.fit(x_train_norm, y_train_cat, epochs=30, validation_split=0.2)
plt.plot(mdl_fit.history["accuracy"], label="training")
plt.plot(mdl_fit.history["val_accuracy"], label="validation")
plt.title("Accuracy for convolutional model")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

print("I have tried the Artificial neural network for this problem But unfortunately couldn't get better accuracy, It performed not really well. "
"\nAfter that, I have tried Convolutional Neural Network (multi-layer). It has multiple Convolutional, relu, and pooling layers and one dense network."
"\nlike this:"
"\n\n-> Convolutional layer + relu"
"\n-> pooling"
"\n-> Convolutional layer + relu"
"\n-> pooling"
"\n-> dense network"
"\n\nIt performs really well, better than other models I have used, I got better val_accuracy in this model. That's why I kept it.")

"""(h)"""

accuaracy = model.evaluate(x_test_norm, y_test_cat)[1]
print("Accuracy: ", accuaracy)

y_predicted = np.argmax(model.predict(x_test_norm), axis=1)
c_matrix = sklearn.metrics.confusion_matrix(y_test, y_predicted)
print(c_matrix)
df_c_matrix = pd.DataFrame(c_matrix, index=[c for c in class_labels], columns=[c for c in class_labels])
plt.figure(figsize=(10, 8))
plt.title("Confusion Matrix graphicial view")
sb.heatmap(df_c_matrix, annot=True)
plt.show()


print("Validation data is needed to first check if the model overfits (if  val_accuracy decreases, the model overfits to training data) "
"\nand secondly to check if the model reaches maximum accuracy without overfitting. At this point, you can stop  training and use the test "
"\ndata to actually test the model on the unknown data.")